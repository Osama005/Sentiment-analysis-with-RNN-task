{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGFDG7mtAU9y3owYOpfypj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DfY34EOLq3gy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re  # Added for regex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/swiggy.csv')\n",
        "print(\"Columns in the dataset:\")\n",
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tujble95tTUI",
        "outputId": "3f39c727-86fc-4332-83b6-8c51b9dc6609"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset:\n",
            "Index(['ID', 'Area', 'City', 'Restaurant Price', 'Avg Rating', 'Total Rating',\n",
            "       'Food Item', 'Food Type', 'Delivery Time', 'Review'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Cleaning and Sentiment Labeling"
      ],
      "metadata": {
        "id": "pPQS2obiy122"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Review\"] = data[\"Review\"].str.lower()\n",
        "data[\"Review\"] = data[\"Review\"].replace(r'[^a-z0-9\\s]', '', regex=True)\n",
        "\n",
        "data['sentiment'] = data['Avg Rating'].apply(lambda x: 1 if x > 3.5 else 0)\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "ypLYpWtKwj75"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization and Padding"
      ],
      "metadata": {
        "id": "QLfb6Ndry90D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000\n",
        "max_length = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(data[\"Review\"])\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(data[\"Review\"]), maxlen=max_length)\n",
        "y = data['sentiment'].values"
      ],
      "metadata": {
        "id": "lkfj5_hMwqfZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Splitting"
      ],
      "metadata": {
        "id": "wHehJ8fJzLzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
        ")"
      ],
      "metadata": {
        "id": "x-ryufoKx-HL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build RNN Model"
      ],
      "metadata": {
        "id": "7SKpX0ODzUA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=max_features, output_dim=16),\n",
        "    SimpleRNN(64, activation='tanh', return_sequences=False),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "RK7WAQCPwx2J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train & Evaluate Model"
      ],
      "metadata": {
        "id": "xRojeXLszYiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {score[1]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gKRAgxoxlFp",
        "outputId": "60155a15-aaee-4476-8c20-e0d6d10a9915"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6759 - loss: 0.6269 - val_accuracy: 0.7156 - val_loss: 0.5964\n",
            "Epoch 2/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.7196 - loss: 0.5945 - val_accuracy: 0.7156 - val_loss: 0.5963\n",
            "Epoch 3/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - accuracy: 0.7127 - loss: 0.5991 - val_accuracy: 0.7156 - val_loss: 0.6077\n",
            "Epoch 4/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - accuracy: 0.7083 - loss: 0.6088 - val_accuracy: 0.7156 - val_loss: 0.5970\n",
            "Epoch 5/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.7127 - loss: 0.5997 - val_accuracy: 0.7156 - val_loss: 0.5964\n",
            "Epoch 6/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7020 - loss: 0.6090 - val_accuracy: 0.7156 - val_loss: 0.5960\n",
            "Epoch 7/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7221 - loss: 0.5904 - val_accuracy: 0.7156 - val_loss: 0.5990\n",
            "Epoch 8/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.7141 - loss: 0.5992 - val_accuracy: 0.7156 - val_loss: 0.5962\n",
            "Epoch 9/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 49ms/step - accuracy: 0.7186 - loss: 0.5943 - val_accuracy: 0.7156 - val_loss: 0.5963\n",
            "Epoch 10/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - accuracy: 0.7249 - loss: 0.5867 - val_accuracy: 0.7156 - val_loss: 0.5956\n",
            "Test accuracy: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicting Sentiment"
      ],
      "metadata": {
        "id": "2obrG0Yczs7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review_text):\n",
        "    text = review_text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_length)\n",
        "\n",
        "    prediction = model.predict(padded)[0][0]\n",
        "    return f\"{'Positive' if prediction >= 0.5 else 'Negative'} (Probability: {prediction:.2f})\"\n",
        "\n",
        "sample_review = \"The food was great.\"\n",
        "print(f\"Review: {sample_review}\")\n",
        "print(f\"Sentiment: {predict_sentiment(sample_review)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SdO47l8x3l5",
        "outputId": "d1252ab2-6e4b-4d65-ea41-64dd887faa1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: The food was great.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n",
            "Sentiment: Positive (Probability: 0.71)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VztVN6cVzx2R"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}